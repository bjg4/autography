---
title: "Speak / See"
author: Blake Green
source_url: https://blake.ist/posts/speak-see
publish_date: '2025-12-17'
scraped_date: '2026-01-19'
---

**Why the future of human-machine interface is already wired to our biology. Yet, we keep installing it backwards**

## I. The Typing Tax

A transformative morning of 2025 started in the rain. Mind racing with stacking ideas while walking a husky-malamute named Winston through Portland rain made typing impossible. Using dictation instead produced results five times faster than thumb-typing—demonstrating how speech represents our oldest and most efficient input method.

Yet nearly all AI products still default to blinking cursors, silently charging users this cognitive cost.

## II. The 10-Bit Funnel

Cognitive science identifies a "conscious bandwidth wall" where roughly one billion sensory bits arrive per second, but only ten bits reach deliberate thought. Speech processes at approximately 39 bits per second—four times faster than typing—because evolution optimized speech production while leaving the higher brain available for environmental scanning.

Vision operates in parallel through separate neural pathways that merge before reaching this bottleneck. Text forces serial processing at both entry and exit, wasting cognitive capacity.

## III. Preattentive Pop-Out and the Red-Dot Economy

Visual systems identify anomalies—like red pixels among gray ones—in thirteen milliseconds, before conscious awareness forms. Stripe's dashboard exemplifies this through single red dots on graphs instead of alert emails, enabling rapid recognition without consuming cognitive resources.

## IV. Tufte's Sparkline and the Vanishing Paragraph

Edward Tufte demonstrated that word-sized graphics replace paragraph-length prose without information loss. Replacing five-sentence summaries with sparklines reduces comprehension time from eighteen seconds to 0.8 seconds, making users feel "psychic" rather than merely informed.

## V. Norman's Button and the Zero-Word Instruction

Don Norman revealed that well-designed objects eliminate instruction requirements. Every tooltip represents failed affordance design and cognitive tax. Superior AI features succeed through removing explanatory text rather than adding model capacity.

## VI. Real-Time Simulation, or the End of "What If"

Human brains learn through parameter variation and observation. Text interrupts this feedback loop with multiple processing steps. Real-time visual simulation—like live color preview on furniture—enables rapid iteration and confident purchasing decisions.

## VII. The Ambiguity Dividend

Speech tolerates imprecision—homophones, hesitations, incomplete thoughts—with free repair. Typing demands upfront precision. Winning products embrace speech ambiguity by surfacing multiple interpretations for user selection rather than forcing exact phrasing.

## VIII. Boundary Lines

Speech-and-visual interfaces fail in specific contexts: passwords, libraries, aircraft, trading floors, and late-night bedrooms. Text input retains advantages where privacy, precision, or silence outweigh speed. Everywhere else, the hierarchy favors voice input and visual output.

## IX. Metrics That Notice the Wall

Four key performance indicators align with cognitive physics:

- **Time-to-insight:** seconds from intent to correct action
- **Cognitive offload:** task completion percentage without scrolling or re-reading
- **Iteration velocity:** parameter adjustments per minute during exploration
- **Error-recovery latency:** milliseconds from ambiguity to confirmed resolution

## X. The Asymmetry Is Not Coming — It Is Waiting

Evolution optimized human biology for rapid warnings and visual pattern recognition. Text-box interfaces fight billion-year-old optimization. Companies recognizing speak-and-see principles create products feeling faster than technically possible while users feel capabilities exceeding actual features. The cognitive tax transfers elsewhere, determining competitive advantage.
