---
title: "Understanding Why There Is Error"
author: Don Norman
source_type: book_chapter
book_title: "The Design of Everyday Things"
chapter_number: 27
scraped_date: '2026-01-19'
---

**Understanding Why There Is Error**


Error occurs for many reasons. The most common is in the nature
of the tasks and procedures that require people to behave in unnatural ways—staying alert for hours at a time, providing precise,
accurate control specifications, all the while multitasking, doing
several things at once, and subjected to multiple interfering activities. Interruptions are a common reason for error, not helped by
designs and procedures that assume full, dedicated attention yet
that do not make it easy to resume operations after an interruption.
And finally, perhaps the worst culprit of all, is the attitude of people toward errors.
When an error causes a financial loss or, worse, leads to an injury
or death, a special committee is convened to investigate the cause
and, almost without fail, guilty people are found. The next step
is to blame and punish them with a monetary fine, or by firing or
jailing them. Sometimes a lesser punishment is proclaimed: make
the guilty parties go through more training. Blame and punish;
blame and train. The investigations and resulting punishments feel


five: _Human Error? No, Bad Design_ **163**


good: “We caught the culprit.” But it doesn’t cure the problem: the
same error will occur over and over again. Instead, when an error
happens, we should determine why, then redesign the product
or the procedures being followed so that it will never occur again
or, if it does, so that it will have minimal impact.


**ROOT CAUSE ANALYSIS**


_Root cause analysis_ is the name of the game: investigate the accident until the single, underlying cause is found. What this ought to
mean is that when people have indeed made erroneous decisions
or actions, we should determine what caused them to err. This is
what root cause analysis ought to be about. Alas, all too often it
stops once a person is found to have acted inappropriately.
Trying to find the cause of an accident sounds good but it is
flawed for two reasons. First, most accidents do not have a single
cause: there are usually multiple things that went wrong, multiple
events that, had any one of them not occurred, would have prevented the accident. This is what James Reason, the noted British
authority on human error, has called the “Swiss cheese model of
accidents” (shown in Figure 5.3 of this chapter on page 208, and
discussed in more detail there).
Second, why does the root cause analysis stop as soon as a human error is found? If a machine stops working, we don’t stop the
analysis when we discover a broken part. Instead, we ask: “Why
did the part break? Was it an inferior part? Were the required specifications too low? Did something apply too high a load on the
part?” We keep asking questions until we are satisfied that we
understand the reasons for the failure: then we set out to remedy
them. We should do the same thing when we find human error:
We should discover what led to the error. When root cause analysis
discovers a human error in the chain, its work has just begun: now
we apply the analysis to understand why the error occurred, and
what can be done to prevent it.
One of the most sophisticated airplanes in the world is the US
Air Force’s F-22. However, it has been involved in a number of
accidents, and pilots have complained that they suffered oxygen


**164** _The Design of Everyday Things_


deprivation (hypoxia). In 2010, a crash destroyed an F-22 and
killed the pilot. The Air Force investigation board studied the incident and two years later, in 2012, released a report that blamed the
accident on pilot error: “failure to recognize and initiate a timely
dive recovery due to channelized attention, breakdown of visual
scan and unrecognized spatial distortion.”
In 2013, the Inspector General’s office of the US Department of
Defense reviewed the Air Force’s findings, disagreeing with the assessment. In my opinion, this time a proper root cause analysis was
done. The Inspector General asked “why sudden incapacitation or
unconsciousness was not considered a contributory factor.” The Air
Force, to nobody’s surprise, disagreed with the criticism. They argued that they had done a thorough review and that their conclusion “was supported by clear and convincing evidence.” Their only
fault was that the report “could have been more clearly written.”
It is only slightly unfair to parody the two reports this way:


_Air Force:_ It was pilot error—the pilot failed to take corrective action.


_Inspector General:_ That’s because the pilot was probably unconscious.


_Air Force:_ So you agree, the pilot failed to correct the problem.


**THE FIVE WHYS**


Root cause analysis is intended to determine the underlying cause
of an incident, not the proximate cause. The Japanese have long
followed a procedure for getting at root causes that they call the
“Five Whys,” originally developed by Sakichi Toyoda and used by
the Toyota Motor Company as part of the Toyota Production System for improving quality. Today it is widely deployed. Basically,
it means that when searching for the reason, even after you have
found one, do not stop: ask why that was the case. And then ask
why again. Keep asking until you have uncovered the true underlying causes. Does it take exactly five? No, but calling the procedure “Five Whys” emphasizes the need to keep going even after a
reason has been found. Consider how this might be applied to the
analysis of the F-22 crash:


five: _Human Error? No, Bad Design_ **165**


**Five Whys**


**Question** **Answer**


Q1: Why did the plane crash? Because it was in an uncontrolled

dive.


Q2: Why didn’t the pilot recover from the dive? Because the pilot failed to initiate a

timely recovery.


Q3: Why was that? Because he might have been
unconscious (or oxygen deprived).


Q4: Why was that? We don’t know. We need to find out.


Etc.


The Five Whys of this example are only a partial analysis. For
example, we need to know why the plane was in a dive (the report
explains this, but it is too technical to go into here; suffice it to say
that it, too, suggests that the dive was related to a possible oxygen
deprivation).
The Five Whys do not guarantee success. The question _why_ is
ambiguous and can lead to different answers by different investigators. There is still a tendency to stop too soon, perhaps when the
limit of the investigator’s understanding has been reached. It also
tends to emphasize the need to find a single cause for an incident,
whereas most complex events have multiple, complex causal factors. Nonetheless, it is a powerful technique.
The tendency to stop seeking reasons as soon as a human error
has been found is widespread. I once reviewed a number of accidents in which highly trained workers at an electric utility company had been electrocuted when they contacted or came too close
to the high-voltage lines they were servicing. All the investigating committees found the workers to be at fault, something even
the workers (those who had survived) did not dispute. But when
the committees were investigating the complex causes of the incidents, why did they stop once they found a human error? Why
didn’t they keep going to find out why the error had occurred,
what circumstances had led to it, and then, why those circumstances had happened? The committees never went far enough to
find the deeper, root causes of the accidents. Nor did they consider
redesigning the systems and procedures to make the incidents


**166** _The Design of Everyday Things_


either impossible or far less likely. When people err, change the
system so that type of error will be reduced or eliminated. When
complete elimination is not possible, redesign to reduce the impact.
It wasn’t difficult for me to suggest simple changes to procedures
that would have prevented most of the incidents at the utility company. It had never occurred to the committee to think of this. The
problem is that to have followed my recommendations would
have meant changing the culture from an attitude among the field
workers that “We are supermen: we can solve any problem, repair
the most complex outage. We do not make errors.” It is not possible to eliminate human error if it is thought of as a personal failure
rather than as a sign of poor design of procedures or equipment.
My report to the company executives was received politely. I was
even thanked. Several years later I contacted a friend at the company and asked what changes they had made. “No changes,” he
said. “And we are still injuring people.”
One big problem is that the natural tendency to blame someone
for an error is even shared by those who made the error, who
often agree that it was their fault. People do tend to blame themselves when they do something that, after the fact, seems inexcusable. “I knew better,” is a common comment by those who
have erred. But when someone says, “It was my fault, I knew
better,” this is not a valid analysis of the problem. That doesn’t
help prevent its recurrence. When many people all have the same
problem, shouldn’t another cause be found? If the system lets you
make the error, it is badly designed. And if the system induces
you to make the error, then it is really badly designed. When I
turn on the wrong stove burner, it is not due to my lack of knowledge: it is due to poor mapping between controls and burners.
Teaching me the relationship will not stop the error from recurring: redesigning the stove will.
We can’t fix problems unless people admit they exist. When
we blame people, it is then difficult to convince organizations to
restructure the design to eliminate these problems. After all, if a
person is at fault, replace the person. But seldom is this the case:
usually the system, the procedures, and social pressures have led


five: _Human Error? No, Bad Design_ **167**


to the problems, and the problems won’t be fixed without addressing all of these factors.
Why do people err? Because the designs focus upon the requirements of the system and the machines, and not upon the requirements of people. Most machines require precise commands
and guidance, forcing people to enter numerical information perfectly. But people aren’t very good at great precision. We frequently
make errors when asked to type or write sequences of numbers
or letters. This is well known: so why are machines still being designed that require such great precision, where pressing the wrong
key can lead to horrendous results?
People are creative, constructive, exploratory beings. We are particularly good at novelty, at creating new ways of doing things,
and at seeing new opportunities. Dull, repetitive, precise requirements fight against these traits. We are alert to changes in the environment, noticing new things, and then thinking about them
and their implications. These are virtues, but they get turned into
negative features when we are forced to serve machines. Then we
are punished for lapses in attention, for deviating from the tightly
prescribed routines.
A major cause of error is time stress. Time is often critical, especially in such places as manufacturing or chemical processing
plants and hospitals. But even everyday tasks can have time pressures. Add environmental factors, such as poor weather or heavy
traffic, and the time stresses increase. In commercial establishments, there is strong pressure not to slow the processes, because
doing so would inconvenience many, lead to significant loss of
money, and, in a hospital, possibly decrease the quality of patient
care. There is a lot of pressure to push ahead with the work even
when an outside observer would say it was dangerous to do so.
In many industries, if the operators actually obeyed all the procedures, the work would never get done. So we push the boundaries:
we stay up far longer than is natural. We try to do too many tasks
at the same time. We drive faster than is safe. Most of the time we

manage okay. We might even be rewarded and praised for our he

**168** _The Design of Everyday Things_


roic efforts. But when things go wrong and we fail, then this same
behavior is blamed and punished.