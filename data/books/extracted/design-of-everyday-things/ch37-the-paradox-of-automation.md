---
title: "The Paradox of Automation"
author: Don Norman
source_type: book_chapter
book_title: "The Design of Everyday Things"
chapter_number: 37
scraped_date: '2026-01-19'
---

**The Paradox of Automation**


Machines are getting smarter. More and more tasks are becoming
fully automated. As this happens, there is a tendency to believe
that many of the difficulties involved with human control will go
away. Across the world, automobile accidents kill and injure tens
of millions of people every year. When we finally have widespread
adoption of self-driving cars, the accident and casualty rate will
probably be dramatically reduced, just as automation in factories
and aviation have increased efficiency while lowering both error
and the rate of injury.
When automation works, it is wonderful, but when it fails, the
resulting impact is usually unexpected and, as a result, dangerous. Today, automation and networked electrical generation systems have dramatically reduced the amount of time that electrical
power is not available to homes and businesses. But when the electrical power grid goes down, it can affect huge sections of a country and take many days to recover. With self-driving cars, I predict
that we will have fewer accidents and injuries, but that when there
is an accident, it will be huge.
Automation keeps getting more and more capable. Automatic
systems can take over tasks that used to be done by people,
whether it is maintaining the proper temperature, automatically
keeping an automobile within its assigned lane at the correct
distance from the car in front, enabling airplanes to fly by themselves from takeoff to landing, or allowing ships to navigate by
themselves. When the automation works, the tasks are usually
done as well as or better than by people. Moreover, it saves people from the dull, dreary routine tasks, allowing more useful,
productive use of time, reducing fatigue and error. But when
the task gets too complex, automation tends to give up. This, of
course, is precisely when it is needed the most. The paradox is
that automation can take over the dull, dreary tasks, but fail with
the complex ones.


five: _Human Error? No, Bad Design_ **213**


When automation fails, it often does so without warning. This is
a situation I have documented very thoroughly in my other books
and many of my papers, as have many other people in the field of
safety and automation. When the failure occurs, the human is “out
of the loop.” This means that the person has not been paying much
attention to the operation, and it takes time for the failure to be
noticed and evaluated, and then to decide how to respond.
In an airplane, when the automation fails, there is usually considerable time for the pilots to understand the situation and respond. Airplanes fly quite high: over 10 km (6 miles) above the
earth, so even if the plane were to start falling, the pilots might
have several minutes to respond. Moreover, pilots are extremely
well trained. When automation fails in an automobile, the person
might have only a fraction of a second to avoid an accident. This
would be extremely difficult even for the most expert driver, and
most drivers are not well trained.

In other circumstances, such as ships, there may be more time
to respond, but only if the failure of the automation is noticed. In
one dramatic case, the grounding of the cruise ship _Royal Majesty_ in
1997, the failure lasted for several days and was only detected in the
postaccident investigation, after the ship had run aground, causing
several million dollars in damage. What happened? The ship’s location was normally determined by the Global Positioning System
(GPS), but the cable that connected the satellite antenna to the navigation system somehow had become disconnected (nobody ever
discovered how). As a result, the navigation system had switched
from using GPS signals to “dead reckoning,” approximating the
ship’s location by estimating speed and direction of travel, but the
design of the navigation system didn’t make this apparent. As a result, as the ship traveled from Bermuda to its destination of Boston,
it went too far south and went aground on Cape Cod, a peninsula
jutting out of the water south of Boston. The automation had performed flawlessly for years, which increased people’s trust and reliance upon it, so the normal manual checking of location or careful
perusal of the display (to see the tiny letters “dr” indicating “dead
reckoning” mode) were not done. This was a huge mode error failure.


**214** _The Design of Everyday Things_