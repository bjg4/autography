---
title: "New Technologies Force Change"
author: Don Norman
source_type: book_chapter
book_title: "The Design of Everyday Things"
chapter_number: 47
scraped_date: '2026-01-19'
---

**New Technologies Force Change**


Today, we have new requirements. We now need to type on small,
portable devices that don’t have room for a full keyboard. Touchand gesture-sensitive screens allow a new form of typing. We can
bypass typing altogether through handwriting recognition and
speech understanding.
Consider the four products shown in Figure 7.2. Their appearance and methods of operations changed radically in their century
of existence. Early telephones, such as the one in Figure 7.2A, did
not have keyboards: a human operator intervened to make the connections. Even when operators were first replaced by automatic
switching systems, the “keyboard” was a rotary dial with ten holes,


**264** _The Design of Everyday Things_


one for each digit. When the dial was replaced with pushbutton
keys, it suffered a slight case of featuritis: the ten positions of the
dial were replaced with twelve keys: the ten digits plus * and #.
But much more interesting is the merger of devices. The human
computer gave rise to laptops, small portable computers. The telephone moved to small, portable cellular phones (called mobiles
in much of the world). Smart phones had large, touch-sensitive
screens, operated by gesture. Soon computers merged into tablets, as did cell phones. Cameras merged with cell phones. Today,
talking, video conferences, writing, photography (both still and
video), and collaborative interaction of all sorts are increasingly


**A.** **B.**


**FIGURE 7.2.** **100 Years of Telephones and Keyboards.** Figures A and B show the
change in the telephone from the Western Electric crank telephone of the 1910s, where
rotating the crank on the right generated a signal alerting the operator, to the phone of
the 2010s. They seem to have nothing in common. Figures C and D contrast a keyboard
of the 1910s with one from the 2010s. The keyboards are still laid out in the same way,
but the first requires physical depression of each key; the second, a quick tracing of a
finger over the relevant letters (the image shows the word _many_ being entered). Credits: A, B, and C: photographs by the author; objects in A and C courtesy of the Museum
of American Heritage, Palo Alto, California. D shows the “Swype” keyboard from
Nuance. Image being used courtesy of Nuance Communications, Inc.


seven: _Design in the World of Business_ **265**


being done by one single device, available with a large variety of
screen sizes, computational power, and portability. It doesn’t make
sense to call them computers, phones, or cameras: we need a new
name. Let’s call them “smart screens.” In the twenty-second century, will we still have phones? I predict that although we will still
talk with one another over a distance, we will not have any device
called a telephone.
As the pressures for larger screens forced the demise of physical keyboards (despite the attempt to make tiny keyboards, operated with single fingers or thumbs), the keyboards were displayed
on the screen whenever needed, each letter tapped one at a time.
This is slow, even when the system tries to predict the word being
typed so that keying can stop as soon as the correct word shows
up. Several systems were soon developed that allowed the finger or
stylus to trace a path among the letters of the word: word-gesture
systems. The gestures were sufficiently different from one another
that it wasn’t even necessary to touch all the letters—it only mattered that the pattern generated by the approximation to the correct path was close enough to the desired one. This turns out to be
a fast and easy way to type (Figure 7.2D).
With gesture-based systems, a major rethinking is possible. Why
keep the letters in the same QWERTY arrangement? The pattern
generation would be even faster if letters were rearranged to maximize speed when using a single finger or stylus to trace out the
letters. Good idea, but when one of the pioneers in developing this
technique, Shumin Zhai, then at IBM, tried it, he ran into the legacy
problem. People knew QWERTY and balked at having to learn a
different organization. Today, the word-gesture method of typing
is widely used, but with QWERTY keyboards (as in Figure 7.2D).
Technology changes the way we do things, but fundamental
needs remain unchanged. The need for getting thoughts written
down, for telling stories, doing critical reviews, or writing fiction
and nonfiction will remain. Some will be written using traditional
keyboards, even on new technological devices, because the keyboard still remains the fastest way to enter words into a system,


**266** _The Design of Everyday Things_


whether it be paper or electronic, physical or virtual. Some people
will prefer to speak their ideas, dictating them. But spoken words
are still likely to be turned into printed words (even if the print
is simply on a display device), because reading is far faster and
superior to listening. Reading can be done quickly: it is possible to
read around three hundred words per minute and to skim, jumping ahead and back, effectively acquiring information at rates in
the thousands of words per minute. Listening is slow and serial,
usually at around sixty words per minute, and although this rate
can be doubled or tripled with speech compression technologies
and training, it is still slower than reading and not easy to skim.
But the new media and new technologies will supplement the old,
so that writing will no longer dominate as much as it did in the
past, when it was the only medium widely available. Now that
anyone can type and dictate, take photographs and videos, draw
animated scenes, and creatively produce experiences that in the
twentieth century required huge amounts of technology and large
crews of specialized workers, the types of devices that allow us to
do these tasks and the ways they are controlled will proliferate.
The role of writing in civilization has changed over its five thousand years of existence. Today, writing has become increasingly
common, although increasingly as short, informal messages. We
now communicate using a wide variety of media: voice, video,
handwriting, and typing, sometimes with all ten fingers, sometimes just with the thumbs, and sometimes by gestures. Over time,
the ways by which we interact and communicate change with technology. But because the fundamental psychology of human beings
will remain unchanged, the design rules in this book will still apply.
Of course, it isn’t just communication and writing that has
changed. Technological change has impacted every sphere of our
lives, from the way education is conducted, to medicine, foods,
clothing, and transportation. We now can manufacture things at
home, using 3-D printers. We can play games with partners around
the world. Cars are capable of driving themselves, and their engines have changed from internal combustion to an assortment of


seven: _Design in the World of Business_ **267**


pure electric and hybrids. Name an industry or an activity and if
it hasn’t already been transformed by new technologies, it will be.
Technology is a powerful driver for change. Sometimes for the
better, sometimes for the worse. Sometimes to fulfill important
needs, and sometimes simply because the technology makes the
change possible.


**How Long Does It Take**
**to Introduce a New Product?**


How long does it take for an idea to become a product? And after
that, how long before the product becomes a long-lasting success?
Inventors and founders of startup companies like to think the interval from idea to success is a single process, with the total measured in months. In fact, it is multiple processes, where the total
time is measured in decades, sometimes centuries.
Technology changes rapidly, but people and culture change
slowly. Change is, therefore, simultaneously rapid and slow. It can
take months to go from invention to product, but then decades—
sometimes many decades—for the product to get accepted. Older
products linger on long after they should have become obsolete,
long after they should have disappeared. Much of daily life is dictated by conventions that are centuries old, that no longer make
any sense, and whose origins have been forgotten by all except the
historian.

Even our most modern technologies follow this time cycle: fast
to be invented, slow to be accepted, even slower to fade away and
die. In the early 2000s, the commercial introduction of gestural control for cell phones, tablets, and computers radically transformed
the way we interacted with our devices. Whereas all previous electronic devices had numerous knobs and buttons on the outside,
physical keyboards, and ways of calling up numerous menus of
commands, scrolling through them, and selecting the desired
command, the new devices eliminated almost all physical controls
and menus.

Was the development of tablets controlled by gestures revolutionary? To most people, yes, but not to technologists.


**268** _The Design of Everyday Things_


Touch-sensitive displays that could detect the positions of simultaneous finger presses (even if by multiple people) had been
in the research laboratories for almost thirty years (these are called
multi touch displays). The first devices were developed by the
University of Toronto in the early 1980s. Mitsubishi developed a
product that it sold to design schools and research laboratories,
in which many of today’s gestures and techniques were being explored. Why did it take so long for these multitouch devices to become successful products? Because it took decades to transform
the research technology into components that were inexpensive
and reliable enough for everyday products. Numerous small
companies tried to manufacture screens, but the first devices
that could handle multiple touches were either very expensive
or unreliable.

There is another problem: the general conservatism of large companies. Most radical ideas fail: large companies are not tolerant
of failure. Small companies can jump in with new, exciting ideas
because if they fail, well, the cost is relatively low. In the world of
high technology, many people get new ideas, gather together a few
friends and early risk-seeking employees, and start a new company to exploit their visions. Most of these companies fail. Only a
few will be successful, either by growing into a larger company or
by being purchased by a large company.
You may be surprised by the large percentage of failures, but
that is only because they are not publicized: we only hear about
the tiny few that become successful. Most startup companies fail,
but failure in the high-tech world of California is not considered
bad. In fact, it is considered a badge of honor, for it means that
the company saw a future potential, took the risk, and tried. Even
though the company failed, the employees learned lessons that
make their next attempt more likely to succeed. Failure can occur
for many reasons: perhaps the marketplace is not ready; perhaps
the technology is not ready for commercialization; perhaps the
company runs out of money before it can gain traction.
When one early startup company, Fingerworks, was struggling
to develop an affordable, reliable touch surface that distinguished


seven: _Design in the World of Business_ **269**


among multiple fingers, it almost quit because it was about to run
out of money. Apple however, anxious to get into this market,
bought Fingerworks. When it became part of Apple, its financial
needs were met and Fingerworks technology became the driving
force behind Apple’s new products. Today, devices controlled by
gestures are everywhere, so this type of interaction seems natural
and obvious, but at the time, it was neither natural nor obvious.

It took almost three decades from the invention of multitouch

before companies were able to manufacture the technology with
the required robustness, versatility, and very low cost necessary
for the idea to be deployed in the home consumer market. Ideas
take a long time to traverse the distance from conception to successful product.


**VIDEOPHONE:**

**CONCEIVED IN 1879—STILL NOT HER E**


The Wikipedia article on videophones, from which Figure 7.3
was taken, said: “George du Maurier’s cartoon of ‘an electric camera-obscura’ is often cited as an early prediction of television and
also anticipated the videophone, in wide screen formats and flat
screens.” Although the title of the drawing gives credit to Thomas
Edison, he had nothing to do with this. This is sometimes called
Stigler’s law: the names of famous people often get attached to
ideas even though they had nothing to do with them.
The world of product design offers many examples of Stigler’s
law. Products are thought to be the invention of the company that
most successfully capitalized upon the idea, not the company that
originated it. In the world of products, original ideas are the easy
part. Actually producing the idea as a successful product is what
is hard. Consider the idea of a video conversation. Thinking of the
idea was so easy that, as we see in Figure 7.3, _Punch_ magazine illustrator du Maurier could draw a picture of what it might look like
only two years after the telephone was invented. The fact that he
could do this probably meant that the idea was already circulating.
By the late 1890s, Alexander Graham Bell had thought through a
number of the design issues. But the wonderful scenario illustrated


**270** _The Design of Everyday Things_


**FIGURE 7.3** **Predicting the Future: The Videophone in 1879.** The caption reads:
“Edison’s Telephonoscope (transmits light as well as sound). ( _Every evening, before go-_
_ing to bed, Pater- and Materfamilias set up an electric camera-obscura over their bedroom_
_mantel-piece, and gladden their eyes with the sight of their children at the Antipodes, and_
_converse gaily with them through the wire._ ”) (Published in the December 9, 1878, issue of _Punch_
magazine. From “Telephonoscope,” Wikipedia.)


by du Maurier has still not become reality, one and one-half centuries later. Today, the videophone is barely getting established as a
means of everyday communication.
It is extremely difficult to develop all the details required to ensure that a new idea works, to say nothing of finding components
that can be manufactured in sufficient quantity, reliability, and affordability. With a brand-new concept, it can take decades before
the public will endorse it. Inventors often believe their new ideas
will revolutionize the world in months, but reality is harsher. Most
new inventions fail, and even the few that succeed take decades

to do so. Yes, even the ones we consider “fast.” Most of the time,
the technology is unnoticed by the public as it circulates around
the research laboratories of the world or is tried by a few unsuccessful startup companies or adventurous early adopters.


seven: _Design in the World of Business_ **271**


Ideas that are too early often fail, even if eventually others introduce them successfully. I’ve seen this happen several times.
When I first joined Apple, I watched as it released one of the very
first commercial digital cameras: the Apple QuickTake. It failed.
Probably you are unaware that Apple ever made cameras. It failed
because the technology was limited, the price high, and the world
simply wasn’t ready to dismiss film and chemical processing of
photographs. I was an adviser to a startup company that produced
the world’s first digital picture frame. It failed. Once again, the
technology didn’t quite support it and the product was relatively
expensive. Obviously today, digital cameras and digital photo
frames are extremely successful products, but neither Apple nor
the startup I worked with are part of the story.
Even as digital cameras started to gain a foothold in photography, it took several decades before they displaced film for still
photographs. It is taking even longer to replace film-based movies with those produced on digital cameras. As I write this, only a
small number of films are made digitally, and only a small number
of theaters project digitally. How long has the effort been going on?
It is difficult to determine when the effort stated, but it has been
a very long time. It took decades for high-definition television to
replace the standard, very poor resolution of the previous generation (NTSC in the United States and PAL and SECAM elsewhere).
Why so long to get to a far better picture, along with far better
sound? People are very conservative. Broadcasting stations would
have to replace all their equipment. Homeowners would need new
sets. Overall, the only people who push for changes of this sort are
the technology enthusiasts and the equipment manufacturers. A
bitter fight between the television broadcasters and the computer
industry, each of which wanted different standards, also delayed
adoption (described in Chapter 6).
In the case of the videophone shown in Figure 7.3, the illustration is wonderful but the details are strangely lacking. Where
would the video camera have to be located to display that wonderful panorama of the children playing? Notice that “Pater- and
Materfamilias” are sitting in the dark (because the video image is


**272** _The Design of Everyday Things_


projected by a “camera obscura,” which has a very weak output).
Where is the video camera that films the parents, and if they sit
in the dark, how can they be visible? It is also interesting that although the video quality looks even better than we could achieve
today, sound is still being picked up by trumpet-shaped telephones
whose users need to hold the speaking tube to their face and talk
(probably loudly). Thinking of the concept of a video connection
was relatively easy. Thinking through the details has been very difficult, and then being able to build it and put it into practice—well,
it is now considerably over a century since that picture was drawn
and we are just barely able to fulfill that dream. Barely.
It took forty years for the first working videophones to be created (in the 1920s), then another ten years before the first product
(in the mid-1930s, in Germany), which failed. The United States
didn’t try commercial videophone service until the 1960s, thirty
years after Germany; that service also failed. All sorts of ideas have
been tried including dedicated videophone instruments, devices
using the home television set, video conferencing with home personal computers, special video-conferencing rooms in universities
and companies, and small video telephones, some of which might
be worn on the wrist. It took until the start of the twenty-first century for usage to pick up.
Video conferencing finally started to become common in the
early 2010s. Extremely expensive videoconferencing suites have
been set up in businesses and universities. The best commercial
systems make it seem as if you are in the same room with the
distant participants, using high-quality transmission of images
and multiple, large monitors to display life-size images of people
sitting across the table (one company, Cisco, even sells the table).
This is 140 years from the first published conception, 90 years
since the first practical demonstration, and 80 years since the first
commercial release. Moreover, the cost, both for the equipment
at each location and for the data-transmission charges, are much
higher than the average person or business can afford: right now
they are mostly used in corporate offices. Many people today do
engage in videoconferencing from their smart display devices,


seven: _Design in the World of Business_ **273**


but the experience is not nearly as good as provided by the best
commercial facilities. Nobody would confuse these experiences
with being in the same room as the participants, something that
the highest-quality commercial facilities aspire to (with remarkable success).
Every modern innovation, especially the ones that significantly
change lives, takes multiple decades to move from concept to company success A rule of thumb is twenty years from first demonstrations in research laboratories to commercial product, and
then a decade or two from first commercial release to widespread
adoption. Except that actually, most innovations fail completely
and never reach the public. Even ideas that are excellent and will
eventually succeed frequently fail when first introduced. I’ve been
associated with a number of products that failed upon introduction, only to be very successful later when reintroduced (by other
companies), the real difference being the timing. Products that
failed at first commercial introduction include the first American

automobile (Duryea), the first typewriters, the first digital cameras,
and the first home computers (for example, the Altair 8800 computer of 1975).


**THE LONG PROCESS OF DEVELOPMENT**

**OF THE TYPEWRITER KEYBOARD**


The typewriter is an ancient mechanical device, now found mostly
in museums, although still in use in newly developing nations.
In addition to having a fascinating history, it illustrates the difficulties of introducing new products into society, the influence of
marketing upon design, and the long, difficult path leading to new
product acceptance. The history affects all of us because the typewriter provided the world with the arrangement of keys on today’s
keyboards, despite the evidence that it is not the most efficient arrangement. Tradition and custom coupled with the large number
of people already used to an existing scheme makes change difficult or even impossible. This is the legacy problem once again: the
heavy momentum of legacy inhibits change.


**274** _The Design of Everyday Things_


Developing the first successful typewriter was a lot more than
simply figuring out a reliable mechanism for imprinting the letters upon the paper, although that was a difficult task by itself.
One question was the user interface: how should the letters be presented to the typist? In other words, the design of the keyboard.
Consider the typewriter keyboard, with its arbitrary, diagonally
sloping arrangement of keys and its even more arbitrary arrangement of their letters. Christopher Latham Sholes designed the current standard keyboard in the 1870s. His typewriter design, with
its weirdly organized keyboard, eventually became the Remington
typewriter, the first successful typewriter: its keyboard layout was
soon adopted by everyone.
The design of the keyboard has a long and peculiar history. Early
typewriters experimented with a wide variety of layouts, using
three basic themes. One was circular, with the letters laid out alphabetically; the operator would find the proper spot and depress
a lever, lift a rod, or do whatever other mechanical operation the
device required. Another popular layout was similar to a piano
keyboard, with the letters laid out in a long row; some of the early
keyboards, including an early version by Sholes, even had black
and white keys. Both the circular layout and the piano keyboard
proved awkward. In the end, the typewriter keyboards all ended
up using multiple rows of keys in a rectangular configuration, with
different companies using different arrangements of the letters.
The levers manipulated by the keys were large and ungainly, and the
size, spacing, and arrangement of the keys were dictated by these
mechanical considerations, not by the characteristics of the human
hand. Hence the keyboard sloped and the keys were laid out in
a diagonal pattern to provide room for the mechanical linkages.
Even though we no longer use mechanical linkages, the keyboard
design is unchanged, even for the most modern electronic devices.
Alphabetical ordering of keys seems logical and sensible: Why
did it change? The reason is rooted in the early technology of keyboards. Early typewriters had long levers attached to the keys.
The levers moved individual typebars to contact the typing paper,


seven: _Design in the World of Business_ **275**


**FIGURE 7.4.** **The 1872 Sholes Typewriter.** Remington, the manufacturer of the
first successful typewriter, also made sewing machines. Figure A shows the influence of the sewing machine upon the design with the use of a foot pedal for
what eventually became the “return” key. A heavy weight hung from the frame
advanced the carriage after each letter was struck, or when the large, rectangular
plate under the typist’s left hand was depressed (this is the “space bar”). Pressing
the foot pedal raised the weight. Figure B shows a blowup of the keyboard. Note
that the second row shows a period (.) instead of R. From _Scientific American’s_ “The
Type Writer” (Anonymous, 1872).


usually from behind (the letters being typed could not be seen
from the front of the typewriter). These long type arms would often collide and lock together, requiring the typist to separate them
manually. To avoid the jamming, Sholes arranged the keys and the
typebars so that letters that were frequently typed in sequence did
not come from adjacent typebars. After a few iterations and experiments, a standard emerged, one that today governs keyboards
used throughout the world, although with regional variations. The
top row of the American keyboard has the keys Q W E R T Y U I O P,
which gives rise to the name of this layout: QWERTY. The world
has adopted the basic layout, although in Europe, for example, one
can find QZERTY, AZERTY, and QWERTZ. Different languages
use different alphabets, so obviously a number of keyboards had
to move keys around to make room for additional characters.
Note that popular legend has it that the keys were placed so as
to slow down the typing. This is wrong: the goal was to have the
mechanical typebars approach one another at large angles, thus
minimizing the chance of collision. In fact, we now know that the


**276** _The Design of Everyday Things_


QWERTY arrangement guarantees a fast typing speed. By placing letters that form frequent pairs relatively far apart, typing is
speeded because it tends to make letter pairs be typed with different hands .

There is an unconfirmed story that a salesperson rearranged
the keyboard to make it possible to type the word _typewriter_ on
the second row, a change that violated the design principle of separating letters that were typed sequentially. Figure 7.4B shows that
the early Sholes keyboard was not QWERTY: the second row of
keys had a period (.) where today we have R, and the P and R keys
were on the bottom row (as well as other differences). Moving the
R and P from the fourth row to the second makes it possible to type
the word _typewriter_ using only keys on the second row.
There is no way to confirm the validity of the story. Moreover,
I have only heard it describe the interchange of the period and R
keys, with no discussion of the P key. For the moment, suppose
the story were true: I can imagine the engineering minds being
outraged. This sounds like the traditional clash between the hardheaded, logical engineers and the noncomprehending sales and
marketing force. Was the salesperson wrong? (Note that today we
would call this a marketing decision, but the profession of marketing didn’t exist yet.) Well, before taking sides, realize that until
then, every typewriter company had failed. Remington was going
to come out with a typewriter with a weird arrangement of the
keys. The sales staff were right to be worried. They were right to
try anything that might enhance the sales efforts. And indeed, they
succeeded: Remington became the leader in typewriters. Actually,
its first model did not succeed. It took quite a while for the public
to accept the typewriter.
Was the keyboard really changed to allow the word _typewriter_
to be typed on one row? I cannot find any solid evidence. But it is
clear that the positions of R and P were moved to the second row:
compare Figure 7.4B with today’s keyboard.
The keyboard was designed through an evolutionary process,
but the main driving forces were mechanical and marketing. Even
though jamming isn’t a possibility with electronic keyboards and


seven: _Design in the World of Business_ **277**


computers and the style of typing has changed, we are committed
to this keyboard, stuck with it forever. But don’t despair: it really
is a good arrangement. One legitimate area of concern is the high
incidence of a kind of injury that befalls typists: carpal tunnel syndrome. This ailment is a result of frequent and prolonged repetitive
motions of the hand and wrist, so it is common among typists,
musicians, and people who do a lot of handwriting, sewing, some
sports, and assembly line work. Gestural keyboards, such as the
one shown in Figure 7.2D, might reduce the incidence. The US National Institute of Health advises, “Ergonomic aids, such as split
keyboards, keyboard trays, typing pads, and wrist braces, may be
used to improve wrist posture during typing. Take frequent breaks
when typing and always stop if there is tingling or pain.”
August Dvorak, an educational psychologist, painstakingly
developed a better keyboard in the 1930s. The Dvorak keyboard
layout is indeed superior to that of QWERTY, but not to the extent
claimed. Studies in my laboratory showed that the typing speed on
a QWERTY was only slightly slower than on a Dvorak, not different enough to make upsetting the legacy worthwhile. Millions of
people would have to learn a new style of typing. Millions of typewriters would have to be changed. Once a standard is in place, the
vested interests of existing practices impede change, even where
the change would be an improvement. Moreover, in the case of
QWERTY versus Dvorak, the gain is simply not worth the pain.
“Good enough” triumphs again.
What about keyboards in alphabetical order? Now that we
no longer have mechanical constraints on keyboard ordering,
wouldn’t they at least be easier to learn? Nope. Because the letters
have to be laid out in several rows, just knowing the alphabet isn’t
enough. You also have to know where the rows break, and today,
every alphabetic keyboard breaks the rows at different points. One
great advantage of QWERTY—that frequent letter pairs are typed
with opposite hands—would no longer be true. In other words,
forget it. In my studies, QWERTY and Dvorak typing speeds were
considerably faster than those on alphabetic keyboards. And an


**278** _The Design of Everyday Things_


alphabetical arrangement of the keys was no faster than a random
arrangement.
Could we do better if we could depress more than one finger at
a time? Yes, court stenographers can out-type anyone else. They
use chord keyboards, typing syllables, not individual letters, directly onto the page—each syllable represented by the simultaneous pressing of keys, each combination being called a “chord.” The
most common keyboard for American law court recorders requires
between two and six keys to be pressed simultaneously to code the
digits, punctuation, and phonetic sounds of English.
Although chord keyboards can be very fast—more than three
hundred words per minute is common—the chords are difficult
to learn and to retain; all the knowledge has to be in the head.
Walk up to any regular keyboard and you can use it right away.
Just search for the letter you want and push that key. With a chord
keyboard, you have to press several keys simultaneously. There is
no way to label the keys properly and no way to know what to do
just by looking. The casual typist is out of luck.


**Two Forms of Innovation:**

**Incremental and Radical**


There are two major forms of product innovation: one follows a
natural, slow evolutionary process; the other is achieved through
radical new development. In general, people tend to think of innovation as being radical, major changes, whereas the most common
and powerful form of it is actually small and incremental.
Although each step of incremental evolution is modest, continual slow, steady improvements can result in rather significant
changes over time. Consider the automobile. Steam-driven vehicles
(the first automobiles) were developed in the late 1700s. The first
commercial automobile was built in 1888 by the German Karl Benz
(his company, Benz & Cie, later merged with Daimler and today is
known as Mercedes-Benz).
Benz’s automobile was a radical innovation. And although his firm
survived, most of its rivals did not. The first American automobile


seven: _Design in the World of Business_ **279**


company was Duryea, which only lasted a few years: being first
does not guarantee success. Although the automobile itself was a
radical innovation, since its introduction it has advanced through
continual slow, steady improvement, year after year: over a century
of incremental innovation (with a few radical changes in components). Because of the century of incremental enhancement, today’s
automobiles are much quieter, faster, more efficient, more comfortable, safer, and less expensive (adjusted for inflation) than those
early vehicles.
Radical innovation changes paradigms. The typewriter was
a radical innovation that had dramatic impact upon office and
home writing. It helped provide a role for women in offices as
typists and secretaries, which led to the redefinition of the job of
secretary to be a dead end rather than the first step toward an
executive position. Similarly, the automobile transformed home
life, allowing people to live at a distance from their work and radically impacting the world of business. It also turned out to be a
massive source of air pollution (although it did eliminate horse
manure from city streets). It is a major cause of accidental death,
with a worldwide fatality rate of over one million each year. The
introduction of electric lighting, the airplane, radio, television,
home computer, and social networks all had massive social impacts. Mobile phones changed the phone industry, and the use of
the technical communication system called packet switching led
to the Internet. These are radical innovations. Radical innovation

changes lives and industries. Incremental innovation makes things
better. We need both.


**INCREMENTAL INNOVATION**


Most design evolves through incremental innovation by means of
continual testing and refinement. In the ideal case, the design is
tested, problem areas are discovered and modified, and then the
product is continually retested and remodified. If a change makes
matters worse, well, it just gets changed again on the next goround. Eventually the bad features are modified into good ones,
while the good ones are kept. The technical term for this process is


**280** _The Design of Everyday Things_


_hill climbing,_ analogous to climbing a hill blindfolded. Move your
foot in one direction. If it is downhill, try another direction. If the
direction is uphill, take one step. Keep doing this until you have
reached a point where all steps would be downhill; then you are at
the top of the hill, or at least at a local peak.
Hill climbing. This method is the secret to incremental innovation. This is at the heart of the human-centered design process discussed in Chapter 6. Does hill climbing always work? Although it
guarantees that the design will reach the top of the hill, what if the
design is not on the best possible hill? Hill climbing cannot find
higher hills: it can only find the peak of the hill it started from.
Want to try a different hill? Try radical innovation, although that is
as likely to find a worse hill as a better one.


**RADICAL INNOVATION**


Incremental innovation starts with existing products and makes
them better. Radical innovation starts fresh, often driven by new
technologies that make possible new capabilities. Thus, the invention of vacuum tubes was a radical innovation, paving the way
for rapid advances in radio and television. Similarly, the invention of the transistor allowed dramatic advances in electronic de
vices, computational power, increased reliability, and lower costs.
The development of GPS satellites unleashed a torrent of locationbased services.

A second factor is the reconsideration of the meaning of technology. Modern data networks serve as an example. Newspapers,
magazines, and books were once thought of as part of the publishing industry, very different from radio and television broadcasting. All of these were different from movies and music. But
once the Internet took hold, along with enhanced and inexpensive
computer power and displays, it became clear that all of these disparate industries were really just different forms of information
providers, so that all could be conveyed to customers by a single
medium. This redefinition collapses together the publishing, telephone, television and cable broadcasting, and music industries. We
still have books, newspapers, and magazines, television shows and


seven: _Design in the World of Business_ **281**


movies, musicians and music, but the way by which they are distributed has changed, thereby requiring massive restructuring of
their corresponding industries. Electronic games, another radical
innovation, are combining with film and video on the one hand,
and books on the other, to form new types of interactive engagement. The collapsing of industries is still taking place, and what
will replace them is not yet clear.
Radical innovation is what many people seek, for it is the big,
spectacular form of change. But most radical ideas fail, and even
those that succeed can take decades and, as this chapter has already illustrated, they may take centuries to succeed. Incremental
product innovation is difficult, but these difficulties pale to insignificance compared to the challenges faced by radical innovation.
Incremental innovations occur by the millions each year; radical
innovation is far less frequent.
What industries are ready for radical innovation? Try education,
transportation, medicine, and housing, all of which are overdue for
major transformation.


**The Design of Everyday Things:**
**1988–2038**


Technology changes rapidly, people and culture change slowly.
Or as the French put it:


_Plus ça change, plus c’est la même chose._
_The more things change, the more they are the same._


Evolutionary change to people is always taking place, but the
pace of human evolutionary change is measured in thousands of
years. Human cultures change somewhat more rapidly over periods measured in decades or centuries. Microcultures, such as the
way by which teenagers differ from adults, can change in a generation. What this means is that although technology is continually
introducing new means of doing things, people are resistant to
changes in the way they do things.


**282** _The Design of Everyday Things_


Consider three simple examples: social interaction, communication, and music. These represent three different human activities,
but each is so fundamental to human life that all three have persisted throughout recorded history and will persist, despite major
changes in the technologies that support these activities. They are
akin to eating: new technologies will change the types of food we
eat and the way it is prepared, but will never eliminate the need
to eat. People often ask me to predict “the next great change.” My
answer is to tell them to examine some fundamentals, such as social interaction, communication, sports and play, music and entertainment. The changes will take place within spheres of activity
such as these. Are these the only fundamentals? Of course not: add
education (and learning), business (and commerce), transportation, self-expression, the arts, and of course, sex. And don’t forget
important sustaining activities, such as the need for good health,
food and drink, clothing, and housing. Fundamental needs will also
stay the same, even if they get satisfied in radically different ways.
_The Design of Everyday Things_ was first published in 1988 (when it
was called _The Psychology of Everyday Things_ ). Since the original publication, technology has changed so much that even though the principles remained constant, many of the examples from 1988 are no
longer relevant. The technology of interaction has changed. Oh yes,
doors and switches, faucets and taps still provide the same difficulties they did back then, but now we have new sources of difficulties
and confusion. The same principles that worked before still apply,
but this time they must also be applied to intelligent machines, to the
continuous interaction with large data sources, to social networks
and to communication systems and products that enable lifelong
interaction with friends and acquaintances across the world.
We gesture and dance to interact with our devices, and in turn
they interact with us via sound and touch, and through multiple
displays of all sizes—some that we wear; some on the floor, walls,
or ceilings; and some projected directly into our eyes. We speak to
our devices and they speak back. And as they get more and more
intelligent, they take over many of the activities we thought that


seven: _Design in the World of Business_ **283**


only people could do. Artificial intelligence pervades our lives and
devices, from our thermostats to our automobiles. Technologies are
always undergoing change.


**AS TECHNOLOGIES CHANGE**

**WILL PEOPLE STAY THE SAME?**


As we develop new forms of interaction and communication,
what new principles are required? What happens when we wear
augmented reality glasses or embed more and more technology
within our bodies? Gestures and body movements are fun, but
not very precise.
For many millennia, even though technology has undergone
radical change, people have remained the same. Will this hold true
in the future? What happens as we add more and more enhancements inside the human body? People with prosthetic limbs will
be faster, stronger, and better runners or sports players than normal players. Implanted hearing devices and artificial lenses and
corneas are already in use. Implanted memory and communication devices will mean that some people will have permanently
enhanced reality, never lacking for information. Implanted computational devices could enhance thinking, problem-solving, and
decision-making. People might become cyborgs: part biology,
part artificial technology. In turn, machines will become more like
people, with neural-like computational abilities and humanlike
behavior. Moreover, new developments in biology might add to
the list of artificial supplements, with genetic modification of people and biological processors and devices for machines.
All of these changes raise considerable ethical issues. The longheld view that even as technology changes, people remain the same
may no longer hold. Moreover, a new species is arising, artificial
devices that have many of the capabilities of animals and people,
sometimes superior abilities. (That machines might be better than
people at some things has long been true: they are clearly stronger and faster. Even the simple desk calculator can do arithmetic
better than we can, which is why we use them. Many computer
programs can do advanced mathematics better than we can, which


**284** _The Design of Everyday Things_


makes them valuable assistants.) People are changing; machines
are changing. This also means that cultures are changing.
There is no question that human culture has been vastly impacted
by the advent of technology. Our lives, our family size and living
arrangements, and the role played by business and education in
our lives are all governed by the technologies of the era. Modern
communication technology changes the nature of joint work. As
some people get advanced cognitive skills due to implants, while
some machines gain enhanced human-qualities through advanced
technologies, artificial intelligence, and perhaps bionic technologies, we can expect even more changes. Technology, people, and
cultures: all will change.


**THINGS THAT MAKE US SMART**


Couple the use of full-body motion and gestures with high-quality
auditory and visual displays that can be superimposed over the
sounds and sights of the world to amplify them, to explain and
annotate them, and we give to people power that exceeds anything
ever known before. What do the limits of human memory mean
when a machine can remind us of all that has happened before, at
precisely the exact time the information is needed? One argument
is that technology makes us smart: we remember far more than
ever before and our cognitive abilities are much enhanced.
Another argument is that technology makes us stupid. Sure, we
look smart with the technology, but take it away and we are worse
off than before it existed. We have become dependent upon our
technologies to navigate the world, to hold intelligent conversation, to write intelligently, and to remember.
Once technology can do our arithmetic, can remember for us,
and can tell us how to behave, then we have no need to learn
these things. But the instant the technology goes away, we are
left helpless, unable to do any basic functions. We are now so
dependent upon technology that when we are deprived, we suffer. We are unable to make our own clothes from plants and animal skins, unable to grow and harvest crops or catch animals.
Without technology, we would starve or freeze to death. Without


seven: _Design in the World of Business_ **285**


cognitive technologies, will we fall into an equivalent state of
ignorance?
These fears have long been with us. In ancient Greece, Plato tells
us that Socrates complained about the impact of books, arguing
that reliance on written material would diminish not only memory
but the very need to think, to debate, to learn through discussion.
After all, said Socrates, when a person tells you something, you
can question the statement, discuss and debate it, thereby enhancing the material and the understanding. With a book, well, what
can you do? You can’t argue back.
But over the years, the human brain has remained much the
same. Human intelligence has certainly not diminished. True,
we no longer learn how to memorize vast amounts of material.
We no longer need to be completely proficient at arithmetic, for
calculators—present as dedicated devices or on almost every
computer or phone—take care of that task for us. But does that
make us stupid? Does the fact that I can no longer remember my
own phone number indicate my growing feebleness? No, on the
contrary, it unleashes the mind from the petty tyranny of tending
to the trivial and allows it to concentrate on the important and
the critical.

Reliance on technology is a benefit to humanity. With technology, the brain gets neither better nor worse. Instead, it is the task
that changes. Human plus machine is more powerful than either
human or machine alone.

The best chess-playing machine can beat the best human chess
player. But guess what, the combination of human plus machine
can beat the best human and the best machine. Moreover, this winning combination need not have the best human or machine. As
MIT professor Erik Brynjolfsson explained at a meeting of the National Academy of Engineering:


_The best chess player in the world today is not a computer or a human_
_but a team of humans and computers working together. In freestyle_
_chess competitions, where teams of humans and computers compete,_


**286** _The Design of Everyday Things_


_the winners tend not to be the teams with the most powerful computers_
_or the best chess players. The winning teams are able to leverage the_
_unique skills of humans and computers to work together. That is a met-_
_aphor for what we can do going forward: have people and technology_
_work together in new ways to create value._ (Brynjolfsson, 2012.)


Why is this? Brynjolfsson and Andrew McAfee quote the
world-champion human chess player Gary Kasparov, explaining
why “the overall winner in a recent freestyle tournament had neither the best human players nor the most powerful computers.”
Kasparov described a team consisting of:


_a pair of amateur American chess players using three computers at the_
_same time. Their skill at manipulating and “coaching” their computers_
_to look very deeply into positions effectively counteracted the superior_
_chess understanding of their grandmaster opponents and the greater_
_computational power of other participants.Weak human + machine +_
_better process was superior to a strong computer alone and, more re-_
_markably, superior to a strong human + machine + inferior process._


(Brynjolfsson & McAfee, 2011.)


Moreover, Brynjolfsson and McAfee argue that the same pattern is
found in many activities, including both business and science: “The
key to winning the race is not to compete against machines but to compete with machines. Fortunately, humans are strongest exactly where
computers are weak, creating a potentially beautiful partnership.”
The cognitive scientist (and anthropologist) Edwin Hutchins of
the University of California, San Diego, has championed the power
of distributed cognition, whereby some components are done by
people (who may be distributed across time and space); other components, by our technologies. It was he who taught me how powerful this combination makes us. This provides the answer to the
question: Does the new technology make us stupid? No, on the
contrary, it changes the tasks we do. Just as the best chess player
is a combination of human and technology, we, in combination


seven: _Design in the World of Business_ **287**


with technology, are smarter than ever before. As I put it in my
book _Things That Make Us Smart,_ the power of the unaided mind is
highly overrated. It is things that make us smart.


_The power of the unaided mind is highly overrated. Without external_
_aids, deep, sustained reasoning is difficult. Unaided memory, thought,_
_and reasoning are all limited in power. Human intelligence is highly_
_flexible and adaptive, superb at inventing procedures and objects that_
_overcome its own limits. The real powers come from devising external_
_aids that enhance cognitive abilities. How have we increased memory,_
_thought and reasoning? By the invention of external aids: it is things_
_that make us smart. Some assistance comes through cooperative, social_
_behavior: some arises through exploitation of the information pres-_
_ent in the environment; and some comes through the development of_
_tools of thought—cognitive artifacts—that complement abilities and_
_strengthen mental powers._ (The opening paragraph of Chapter 3, T _hings_


_That Make Us Smart_, 1993.)