---
title: "Reporting Error"
author: Don Norman
source_type: book_chapter
book_title: "The Design of Everyday Things"
chapter_number: 33
scraped_date: '2026-01-19'
---

**Reporting Error**


If errors can be caught, then many of the problems they might lead
to can often be avoided. But not all errors are easy to detect. Moreover, social pressures often make it difficult for people to admit to


five: _Human Error? No, Bad Design_ **191**


their own errors (or to report the errors of others). If people report
their own errors, they might be fined or punished. Moreover, their
friends may make fun of them. If a person reports that someone
else made an error, this may lead to severe personal repercussions.
Finally, most institutions do not wish to reveal errors made by their
staff. Hospitals, courts, police systems, utility companies—all are
reluctant to admit to the public that their workers are capable of
error. These are all unfortunate attitudes.

The only way to reduce the incidence of errors is to admit their
existence, to gather together information about them, and thereby
to be able to make the appropriate changes to reduce their occurrence. In the absence of data, it is difficult or impossible to make
improvements. Rather than stigmatize those who admit to error,
we should thank those who do so and encourage the reporting.
We need to make it easier to report errors, for the goal is not to
punish, but to determine how it occurred and change things so
that it will not happen again.


**CASE STUDY:** _**JIDOKA**_ **—HOW TOYOTA HANDLES ERROR**


The Toyota automobile company has developed an extremely efficient error-reduction process for manufacturing, widely known as
the Toyota Production System. Among its many key principles is a
philosophy called _Jidoka,_ which Toyota says is “roughly translated
as ‘automation with a human touch.’” If a worker notices some
thing wrong, the worker is supposed to report it, sometimes even
stopping the entire assembly line if a faulty part is about to proceed to the next station. (A special cord, called an _andon,_ stops the
assembly line and alerts the expert crew.) Experts converge upon
the problem area to determine the cause. “Why did it happen?”
“Why was that?” “Why is that the reason?” The philosophy is to
ask “Why?” as many times as may be necessary to get to the root
cause of the problem and then fix it so it can never occur again.
As you might imagine, this can be rather discomforting for the
person who found the error. But the report is expected, and when
it is discovered that people have failed to report errors, they are
punished, all in an attempt to get the workers to be honest.


**192** _The Design of Everyday Things_


**POKA-YOKE: ERROR PROOFING**


Poka-yoke is another Japanese method, this one invented by Shigeo Shingo, one of the Japanese engineers who played a major role
in the development of the Toyota Production System. _Poka-yoke_
translates as “error proofing” or “avoiding error.” One of the techniques of poka-yoke is to add simple fixtures, jigs, or devices to
constrain the operations so that they are correct. I practice this myself in my home. One trivial example is a device to help me remember which way to turn the key on the many doors in the apartment
complex where I live. I went around with a pile of small, circular,
green stick-on dots and put them on each door beside its keyhole,
with the green dot indicating the direction in which the key needed
to be turned: I added signifiers to the doors. Is this a major error?
No. But eliminating it has proven to be convenient. (Neighbors
have commented on their utility, wondering who put them there.)
In manufacturing facilities, poka-yoke might be a piece of wood
to help align a part properly, or perhaps plates designed with
asymmetrical screw holes so that the plate could fit in only one position. Covering emergency or critical switches with a cover to prevent accidental triggering is another poka-yoke technique: this is
obviously a forcing function. All the poka-yoke techniques involve
a combination of the principles discussed in this book: affordances,
signifiers, mapping, and constraints, and perhaps most important
of all, forcing functions.


**NASA’S AVIATION SAFETY REPORTING SYSTEM**


US commercial aviation has long had an extremely effective system for encouraging pilots to submit reports of errors. The program has resulted in numerous improvements to aviation safety.
It wasn’t easy to establish: pilots had severe self-induced social
pressures against admitting to errors. Moreover, to whom would
they report them? Certainly not to their employers. Not even to the
Federal Aviation Authority (FAA), for then they would probably
be punished. The solution was to let the National Aeronautics and
Space Administration (NASA) set up a voluntary accident reporting system whereby pilots could submit semi-anonymous reports


five: _Human Error? No, Bad Design_ **193**


of errors they had made or observed in others (semi-anonymous
because pilots put their name and contact information on the reports so that NASA could call to request more information). Once
NASA personnel had acquired the necessary information, they
would detach the contact information from the report and mail it
back to the pilot. This meant that NASA no longer knew who had
reported the error, which made it impossible for the airline companies or the FAA (which enforced penalties against errors) to find
out who had submitted the report. If the FAA had independently
noticed the error and tried to invoke a civil penalty or certificate
suspension, the receipt of self-report automatically exempted the
pilot from punishment (for minor infractions).
When a sufficient number of similar errors had been collected,
NASA would analyze them and issue reports and recommendations to the airlines and to the FAA. These reports also helped
the pilots realize that their error reports were valuable tools for
increasing safety. As with checklists, we need similar systems in
the field of medicine, but it has not been easy to set up. NASA is a
neutral body, charged with enhancing aviation safety, but has no
oversight authority, which helped gain the trust of pilots. There is
no comparable institution in medicine: physicians are afraid that
self-reported errors might lead them to lose their license or be subjected to lawsuits. But we can’t eliminate errors unless we know
what they are. The medical field is starting to make progress, but it
is a difficult technical, political, legal, and social problem.