---
title: "The Classification of Mistakes"
author: Don Norman
source_type: book_chapter
book_title: "The Design of Everyday Things"
chapter_number: 31
scraped_date: '2026-01-19'
---

**The Classification of Mistakes**


Mistakes result from the choice of inappropriate goals and plans or
from faulty comparison of the outcome with the goals during evaluation. In mistakes, a person makes a poor decision, misclassifies a
situation, or fails to take all the relevant factors into account. Many
mistakes arise from the vagaries of human thought, often because
people tend to rely upon remembered experiences rather than on
more systematic analysis. We make decisions based upon what is
in our memory. But as discussed in Chapter 3, retrieval from longterm memory is actually a reconstruction rather than an accurate
record. As a result, it is subject to numerous biases. Among other
things, our memories tend to be biased toward overgeneralization
of the commonplace and overemphasis of the discrepant.
The Danish engineer Jens Rasmussen distinguished among three
modes of behavior: skill-based, rule-based, and knowledge-based.
This three-level classification scheme provides a practical tool that
has found wide acceptance in applied areas, such as the design of


five: _Human Error? No, Bad Design_ **179**


many industrial systems. Skill-based behavior occurs when workers are extremely expert at their jobs, so they can do the everyday,
routine tasks with little or no thought or conscious attention. The
most common form of errors in skill-based behavior is slips.
Rule-based behavior occurs when the normal routine is no lon
ger applicable but the new situation is one that is known, so there
is already a well-prescribed course of action: a rule. Rules simply
might be learned behaviors from previous experiences, but includes formal procedures prescribed in courses and manuals, usually in the form of “if-then” statements, such as, “ _If_ the engine will
not start, _then_ do [the appropriate action].” Errors with rule-based
behavior can be either a mistake or a slip. If the wrong rule is selected, this would be a mistake. If the error occurs during the execution of the rule, it is most likely a slip.
Knowledge-based procedures occur when unfamiliar events occur, where neither existing skills nor rules apply. In this case, there
must be considerable reasoning and problem-solving. Plans might
be developed, tested, and then used or modified. Here, conceptual
models are essential in guiding development of the plan and interpretation of the situation.
In both rule-based and knowledge-based situations, the most serious mistakes occur when the situation is misdiagnosed. As a result,
an inappropriate rule is executed, or in the case of knowledge-based
problems, the effort is addressed to solving the wrong problem. In
addition, with misdiagnosis of the problem comes misinterpretation of the environment, as well as faulty comparisons of the current state with expectations. These kinds of mistakes can be very
difficult to detect and correct.


**RULE-BASED MISTAKES**


When new procedures have to be invoked or when simple problems arise, we can characterize the actions of skilled people as rulebased. Some rules come from experience; others are formal procedures in manuals or rulebooks, or even less formal guides, such
as cookbooks for food preparation. In either case, all we must do
is identify the situation, select the proper rule, and then follow it.


**180** _The Design of Everyday Things_


When driving, behavior follows well-learned rules. Is the light
red? If so, stop the car. Wish to turn left? Signal the intention to
turn and move as far left as legally permitted: slow the vehicle and
wait for a safe break in traffic, all the while following the traffic
rules and relevant signs and lights.
Rule-based mistakes occur in multiple ways:


 - The situation is mistakenly interpreted, thereby invoking the wrong
goal or plan, leading to following an inappropriate rule.

 - The correct rule is invoked, but the rule itself is faulty, either because
it was formulated improperly or because conditions are different
than assumed by the rule or through incomplete knowledge used to
determine the rule. All of these lead to knowledge-based mistakes.

 - The correct rule is invoked, but the outcome is incorrectly evaluated.
This error in evaluation, usually rule- or knowledge-based itself, can
lead to further problems as the action cycle continues.


**Example 1** : In 2013, at the Kiss nightclub in Santa Maria, Brazil, pyrotechnics used by the band ignited a fire that killed over 230 people.
The tragedy illustrates several mistakes. The band made a knowledge-based mistake when they used outdoor flares, which ignited the
ceiling’s acoustic tiles. The band thought the flares were safe. Many
people rushed into the rest rooms, mistakenly thinking they were exits: they died. Early reports suggested that the guards, unaware of
the fire, at first mistakenly blocked people from leaving the building.
Why? Because nightclub attendees would sometimes leave without
paying for their drinks.
The mistake was in devising a rule that did not take account of
emergencies. A root cause analysis would reveal that the goal was
to prevent inappropriate exit but still allow the doors to be used in
an emergency. One solution is doors that trigger alarms when used,
deterring people trying to sneak out, but allowing exit when needed.
**Example 2** : Turning the thermostat of an oven to its maximum temperature to get it to the proper cooking temperature faster is a mistake
based upon a false conceptual model of the way the oven works. If
the person wanders off and forgets to come back and check the oven


five: _Human Error? No, Bad Design_ **181**


temperature after a reasonable period (a memory-lapse slip), the improper high setting of the oven temperature can lead to an accident,
possibly a fire.
**Example 3** : A driver, unaccustomed to anti-lock brakes, encounters
an unexpected object in the road on a wet, rainy day. The driver applies full force to the brakes but the car skids, triggering the anti-lock
brakes to rapidly turn the brakes on and off, as they are designed to
do. The driver, feeling the vibrations, believes that it indicates malfunction and therefore lifts his foot off the brake pedal. In fact, the
vibration is a signal that anti-lock brakes are working properly. The
driver’s misevaluation leads to the wrong behavior.


Rule-based mistakes are difficult to avoid and then difficult to

detect. Once the situation has been classified, the selection of the
appropriate rule is often straightforward. But what if the classification of the situation is wrong? This is difficult to discover because
there is usually considerable evidence to support the erroneous
classification of the situation and the choice of rule. In complex
situations, the problem is too much information: information that
both supports the decision and also contradicts it. In the face of
time pressures to make a decision, it is difficult to know which
evidence to consider, which to reject. People usually decide by taking the current situation and matching it with something that happened earlier. Although human memory is quite good at matching
examples from the past with the present situation, this doesn’t
mean that the matching is accurate or appropriate. The matching
is biased by recency, regularity, and uniqueness. Recent events are
remembered far better than less recent ones. Frequent events
are remembered through their regularities, and unique events are
remembered because of their uniqueness. But suppose the current
event is different from all that has been experienced before: people
are still apt to find some match in memory to use as a guide. The
same powers that make us so good at dealing with the common
and the unique lead to severe error with novel events.
What is a designer to do? Provide as much guidance as possible
to ensure that the current state of things is displayed in a coherent


**182** _The Design of Everyday Things_


and easily interpreted format—ideally graphical. This is a difficult
problem. All major decision makers worry about the complexity
of real-world events, where the problem is often too much information, much of it contradictory. Often, decisions must be made
quickly. Sometimes it isn’t even clear that there is an incident or
that a decision is actually being made.
Think of it like this. In your home, there are probably a number
of broken or misbehaving items. There might be some burnt-out
lights, or (in my home) a reading light that works fine for a little
while, then goes out: we have to walk over and wiggle the fluorescent bulb. There might be a leaky faucet or other minor faults
that you know about but are postponing action to remedy. Now
consider a major process-control manufacturing plant (an oil refinery, a chemical plant, or a nuclear power plant). These have thousands, perhaps tens of thousands, of valves and gauges, displays
and controls, and so on. Even the best of plants always has some
faulty parts. The maintenance crews always have a list of items to
take care of. With all the alarms that trigger when a problem arises,
even though it might be minor, and all the everyday failures, how
does one know which might be a significant indicator of a major
problem? Every single one usually has a simple, rational explanation, so not making it an urgent item is a sensible decision. In fact,
the maintenance crew simply adds it to a list. Most of the time, this is
the correct decision. The one time in a thousand (or even, one time
in a million) that the decision is wrong makes it the one they will
be blamed for: how could they have missed such obvious signals?
Hindsight is always superior to foresight. When the accident investigation committee reviews the event that contributed to the
problem, they know what actually happened, so it is easy for them
to pick out which information was relevant, which was not. This is
retrospective decision making. But when the incident was taking
place, the people were probably overwhelmed with far too much
irrelevant information and probably not a lot of relevant information. How were they to know which to attend to and which to
ignore? Most of the time, experienced operators get things right.
The one time they fail, the retrospective analysis is apt to condemn


five: _Human Error? No, Bad Design_ **183**


them for missing the obvious. Well, during the event, nothing may
be obvious. I return to this topic later in the chapter.
You will face this while driving, while handling your finances,
and while just going through your daily life. Most of the unusual
incidents you read about are not relevant to you, so you can safely
ignore them. Which things should be paid attention to, which
should be ignored? Industry faces this problem all the time, as do
governments. The intelligence communities are swamped with
data. How do they decide which cases are serious? The public
hears about their mistakes, but not about the far more frequent
cases that they got right or about the times they ignored data as not
being meaningful—and were correct to do so.
If every decision had to be questioned, nothing would ever get
done. But if decisions are not questioned, there will be major
mistakes—rarely, but often of substantial penalty.
The design challenge is to present the information about the state
of the system (a device, vehicle, plant, or activities being monitored) in a way that is easy to assimilate and interpret, as well as to
provide alternative explanations and interpretations. It is useful
to question decisions, but impossible to do so if every action—or
failure to act—requires close attention.
This is a difficult problem with no obvious solution.


**KNOWLEDGE-BASED MISTAKES**


Knowledge-based behavior takes place when the situation is novel
enough that there are no skills or rules to cover it. In this case, a
new procedure must be devised. Whereas skills and rules are controlled at the behavioral level of human processing and are therefore subconscious and automatic, knowledge-based behavior is
controlled at the reflective level and is slow and conscious.

With knowledge-based behavior, people are consciously problem solving. They are in an unknown situation and do not have
any available skills or rules that apply directly. Knowledge-based
behavior is required either when a person encounters an unknown
situation, perhaps being asked to use some novel equipment, or


**184** _The Design of Everyday Things_


even when doing a familiar task and things go wrong, leading to a
novel, uninterpretable state.
The best solution to knowledge-based situations is to be found
in a good understanding of the situation, which in most cases also
translates into an appropriate conceptual model. In complex cases,
help is needed, and here is where good cooperative problem-solving
skills and tools are required. Sometimes, good procedural manuals
(paper or electronic) will do the job, especially if critical observations can be used to arrive at the relevant procedures to follow. A
more powerful approach is to develop intelligent computer systems, using good search and appropriate reasoning techniques
(artificial-intelligence decision-making and problem-solving). The
difficulties here are in establishing the interaction of the people with
the automation: human teams and automated systems have to be
thought of as collaborative, cooperative systems. Instead, they are
often built by assigning the tasks that machines can do to the machines and leaving the humans to do the rest. This usually means
that machines do the parts that are easy for people, but when the
problems become complex, which is precisely when people could
use assistance, that is when the machines usually fail. (I discuss
this problem extensively in _The_ _Design of Future Things._ )


**MEMORY-LAPSE MISTAKES**


Memory lapses can lead to mistakes if the memory failure leads to
forgetting the goal or plan of action. A common cause of the lapse
is an interruption that leads to forgetting the evaluation of the current state of the environment. These lead to mistakes, not slips, because the goals and plans become wrong. Forgetting earlier evaluations often means remaking the decision, sometimes erroneously.
The design cures for memory-lapse mistakes are the same as for
memory-lapse slips: ensure that all the relevant information is continuously available. The goals, plans, and current evaluation of
the system are of particular importance and should be continually
available. Far too many designs eliminate all signs of these items
once they have been made or acted upon. Once again, the designer


five: _Human Error? No, Bad Design_ **185**


should assume that people will be interrupted during their activities
and that they may need assistance in resuming their operations.