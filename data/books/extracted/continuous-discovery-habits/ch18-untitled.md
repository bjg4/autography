---
title: "Untitled"
author: Teresa Torres
source_type: book_chapter
book_title: "Continuous Discovery Habits"
chapter_number: 18
scraped_date: '2026-01-19'
---

CHAPTER TEN

TESTING ASSUMPTIONS, NOT IDEAS

“Good tests kill flawed theories; we remain alive to guess again.”

— Karl Popper

“Each answer a team collects—positive or negative—is a unit of progress”

— Jeff Gothelf and Josh Seiden, Sense & Respond

Armed with your “leap of faith” assumptions for three ideas, you might be tempted to rush into assumption testing. It’s exhilarating to get an experiment live and start collecting data. In my coaching program, teams learn to identify assumptions one week and then learn to test assumptions in the following week. Sometimes teams get so excited about testing assumptions that they come to their session during the identifying-assumptions week and report that they’ve already started collecting data on some of their top assumptions.

I admire these teams’ bias toward action. But more often than not, we find problems with their assumption tests. Sometimes their tests aren’t designed to test their “leap of faith” assumption but instead are designed to test the whole idea. Even after doing all the work to identify our riskiest assumptions, it’s easy to get distracted by our great ideas. Sometimes the team hasn’t agreed on what success looks like upfront, and they aren’t sure how to interpret their results. Sometimes they test with the wrong audience, or they get distracted by interesting, but not meaningful, data. These teams are all smart, capable, motivated product trios. However, it’s easy to rush into experimenting before we are ready.

When we rush our experiments, we tend to throw spaghetti at the wall, hoping something sticks. We try variations with abandon, instead of systematically searching for our best option. We run countless tests with little impact. We forget to clearly define what we are trying to learn and what success looks like, leaving us with ambiguous results.

If you’ve ever run an experiment and weren’t sure how to interpret the results, or if you’ve ever wondered if your prototype feedback was good enough, this chapter is for you. You’ll learn how to slow down (just a little bit) to make sure you get more value from each and every assumption test.

Working With Sets of Ideas

As we start assumption testing, we want to make sure that we carry forward the idea of comparing and contrasting our ideas against each other. It’s easy to overcommit to our favorite idea. Why do the work to test three ideas when the best idea looks so good? This is where we want to take a minute to remind ourselves of the cognitive biases we discussed in the previous chapter—confirmation bias and the escalation of commitment.

If we test only one idea at a time, confirmation bias will rear its ugly head. We’ll be more likely to notice the confirming evidence and miss the disconfirming evidence. Similarly, the more time we invest in a single idea, thanks to the escalation of commitment, the more likely we’ll be all-in, committing to the idea, even if it has flaws.

Instead, we want to systematically collect evidence about our assumptions underlying all three ideas. The more we learn about each idea, the more likely we are to compare and contrast the ideas against each other. This helps us make better decisions about which ideas are most promising. Remember—we are looking for a clear front runner.

As you read through the rest of this chapter, remember: We aren’t testing one idea at a time. We are testing assumptions from a set of ideas.

Simulate an Experience, Evaluate Behavior

With assumption testing, our goal is to collect data that will help us move the assumption from the right to the left on our assumption map (see Chapter 9)—we are starting with an assumption that has weak supporting evidence, and our goal is to collect more evidence. Just like with interviewing (see Chapter 5), to collect reliable data, we want to focus on collecting data about what people actually do in a particular context, not just what they think or say they do in general.

A strong assumption test simulates an experience, giving your participant the opportunity to behave either in accordance with your assumption or not. This behavior is what allows us to evaluate our assumption.

To construct a good assumption test, you’ll want to think carefully about the right moment to simulate. You don’t want to simulate any more than you need to. This is what allows you to iterate quickly through several assumption tests.

Let’s return to our streaming-entertainment example, where we are trying to address the target opportunity “I want to watch sports” and we’ve brainstormed a set of solutions—adding local channels, licensing events directly from sports leagues, and bundling our service with a sports provider.

If we are testing the assumption, “Our subscribers want to watch sports,” we might simulate the moment when someone is browsing their streaming options, trying to decide what to watch. We could do this by mocking up the “home screen” that users see when they turn on the streaming-entertainment service. We could present them with several options, including a handful of sporting events, popular TV shows, and recent movie releases. We could then ask them, “What would you prefer to watch right now?”

If we are trying to test the assumption “Our subscriber wants to watch sports on our platform,” we might simulate the moment in time when the big game is about to start. Since our platform doesn’t currently offer sports, we can’t simply ask them which service they prefer. Instead, we might present them with three subscription services (including ours), tell them that the game is available on all three services, and ask them to choose a service to stream the game.

Now, neither of these simulations is perfect. What I say I want to watch when talking to a stranger might differ from what I want to watch when I’m at home by myself. Or I might favor one subscription on one day and another subscription on another day. That’s okay. Perfect simulations are hard to come by. Instead, we’ll account for these shortcomings when we decide how to evaluate the results of our simulation.

Notice how all three ideas depend on the assumption “Our subscriber wants to watch sports.” This is an assumption that is core to the target opportunity, so if this assumption is false, we can abandon our set of ideas. However, only the first and second ideas depend on the assumption “Our subscribers want to watch sports on our platform.” It’s common for ideas to share assumptions. It’s one of the reasons why assumption testing is faster than idea testing. Assumption tests don’t merely give us a go/no-go decision for an individual idea; they help us evaluate sets of ideas. We’ll also see later how shared assumptions will help us generate even better ideas after a round or two of testing.

Once we’ve identified the moment in time that we want to simulate, we now need to define how we’ll evaluate the behavior we observe. Our goal when considering our simulation evaluation is to define what success looks like. In other words, if our assumption is true, what would we expect the participant to do?

For example, when observing people selecting what to watch, we might evaluate how many people choose to watch a sporting event vs. how many people don’t. If our assumption is true—that our subscribers do want to watch sports—we would expect at least some of them to choose a sporting event in our simulation.

When simulating the moment before the big game starts, we might want to evaluate how many people think of our platform as the place to watch the game vs. another platform. If our assumption is true—that our subscribers did want to watch sports on our platform—then we would expect some of them to choose our service over the competitors’.

Now, in both simulations, we say “some people” should exhibit the behavior we expect. The problem with “some” is that your product manager might define “some” as 5 out of 10, and your designer might define “some” as 20 out of 100.

If you run your simulation with 10 people, and 6 people choose sports, your product manager is going to think your assumption is now more known, and your designer is still going to be skeptical. The challenge with this scenario is that, as a team, you didn’t learn anything new from this assumption test because you disagree on what the results mean.

To avoid this situation, we want to get specific with our evaluation criteria. Instead of saying, “Some people choose sports,” we want to say, “At least 3 out of 10 people choose sports.” We want to define both how many people we’ll test with and how many people need to exhibit the behavior that we expect to see.

By defining these criteria upfront, you are doing two things. First, you are aligning as a team around what success looks like so that you all know how to interpret the results. This will help to ensure that your assumption tests are actionable. And second, you are helping to guard against confirmation bias. Remember, confirmation bias makes us more likely to see the evidence that our idea will succeed than the evidence that it might not succeed. If we don’t define our success criteria upfront, when we try to interpret the results, our brains will actively look for evidence that supports the assumption, and we’ll likely miss the evidence that could refute it. To avoid this, we want to define what success looks like upfront (before we see the results).

So how do we choose the numbers? This is a subjective decision. Your goal is to find the right balance between speed of testing and what aligns your team around an actionable outcome. You want to test your assumption with as few people as possible (as it will be faster) but with the number of people that still gives your team the information they need to act on the data. Now remember, you aren’t trying to prove that this assumption is true. The burden of truth is too much. You are simply trying to reduce risk. Keep your assumption map in mind. Your goal is to move the assumption from right to left. How many people would convince you this assumption is more known? That’s the negotiation you are having as a team.

If your simulation is less than optimal, as we saw with the above examples, you’ll need to modify your numbers to accommodate for these shortcomings. If someone raises the concern that some sports fans might want to watch sports on our platform, but in the moment we ask them, they might be more likely to choose a comedy, then you might lower your threshold for success to account for that. If someone else is worried that choosing from three subscriptions biases the results in favor of your subscription (because the reality is people have, on average, five subscription services), then you could either decide to change your mockup to show five services or raise your threshold for your success criteria.

The key outcome with this exercise is to agree as a team on the smallest assumption test you can design that still gets you results that the team will feel comfortable acting on.

Early Signals vs. Large-Scale Experiments

Inevitably, someone on your team is going to raise a concern with making decisions based on small numbers. How can we have confidence in the data if we talk to only five customers? You might be tempted to test with larger pools of people to help get buy-in. But this strategy comes at a cost—it takes more time. We don’t want to invest the time, energy, and effort into an experiment if we don’t even have an early signal that we are on the right track.

Rather than starting with a large-scale experiment (e.g., surveying hundreds of customers, launching a production-quality A/B test, worrying about representative samples), we want to start small. You’ll be pleasantly surprised by how much you can learn from getting feedback from a handful of customers.

Imagine we test the assumption, “Our subscribers want to watch sports,” as described above. We show participants a mockup of our “home screen,” and we ask them what they’d like to watch. Four out of ten choose a sporting event, soaring past the threshold we set for our success criteria. What will we do next?

It depends. We’ve made this assumption more known. However, we can’t conclude it’s true. It still carries risk. The question becomes, “How much risk?” If we have another assumption on our assumption map that is now riskier, we want to switch gears and test that assumption. But if this assumption continues to be our riskiest assumption, and it carries more risk than our organization can stomach, then we need to continue to test it. We need to start defining the next-level experiment that will allow us to collect more data.

Perhaps, as a next step, we decide to add a section to our real “home screen” promoting an upcoming sporting event. When users select it, it informs them that we are considering adding sports to our lineup, and we ask for feedback by way of a thumbs-up or a thumbs-down. We also give them the option to submit comments. We think we can get this experiment live with a week of development work. We decide to collect data from 500 participants (which we think we can do in 3 days), and we set our success criteria to at least 100 of the 500 participants giving us a thumbs-up. This is a classic smoke-screen test.

So why didn’t we start here? Our first test was designed to be completed in a day or two. This test will take up to two weeks—maybe longer, if we need to get permission from stakeholders and/or need to wait for an upcoming release cycle. We don’t want to invest this time, energy, and effort until we’ve received an early signal that we are on the right track.

With assumption testing, most of our learning comes from failed tests. That’s when we learn that something we thought was true might not be. Small tests give us a chance to fail sooner. Failing faster is what allows us to quickly move on to the next assumption, idea, or opportunity. Karl Popper, a renowned 20th-century philosopher of science, in the opening quote argues, “Good tests kill flawed theories,” preventing us from investing where there is little reward, and “we remain alive to guess again,” giving us another chance to get it right.

As we test assumptions, we want to start small and iterate our way to bigger, more reliable, more sound tests, only after each previous round provides an indicator that continuing to invest is worth our effort. We stop testing when we’ve removed enough risk and/or the effort to run the next test is so great that it makes more sense to simply build the idea.

Understanding False Positives and False Negatives

Now, this method isn’t flawless. When working with small numbers, we will encounter false positives and false negatives. Let’s explore the impact of these errors on our work.

In your first round of experimenting, it is possible that you’ll select 10 participants who all hate sports. We can mitigate the risk of this by choosing a variety of folks. In other words, we don’t want to choose 10 participants from Honolulu, Hawaii (where no major sports teams reside), and expect to get reliable results. Instead, we want to select for variation in geographic location, demographics, TV-watching behavior, etc., as best we can. However, even if we select for variation, it is still possible that none of our participants like sports when our larger population does. That’s because we aren’t doing the work to select a representative sample, and we aren’t testing with large-enough numbers.

When this happens, when our experiment fails, even though our larger population exhibits the behavior that we want to see, we call this a “false negative.” Our test is providing data that indicates our assumption is faulty when it may not be.

But what’s the cost of this false negative? In this particular example, where our assumption is testing our target opportunity “Our subscribers want to watch sports,” we might consider abandoning the opportunity. However, we aren’t likely to make this decision based on one failed test. Instead, if we are running tests across our set of ideas, we will have additional data points to help us evaluate the target opportunity.

For example, if we test assumptions across three different ideas, all exploring if our subscribers are interested in sports, and all of them fail, then the chance that all of them are false negatives goes down. More likely, we’ll get conflicting results. We’ll see one assumption fail and another one pass. We’ll need to dig in to learn why. In the worst-case scenario, one of our results will be a false negative, and we’ll have to run additional experiments to evaluate our assumption. However, if our tests are small, this costs us only a day or two. This isn’t a very costly false negative.

Most of our assumptions, however, aren’t testing the opportunity. They are testing some aspect of a particular solution. When these assumptions fail, we typically design around them. We evolve our ideas so that they no longer depend on the faulty assumption. For example, if we are testing the assumption “Our subscribers know where to find sports on our platform,” and it turns out to be problematic, we can always redesign the interface to make sports easier to find. If our failure was a false negative, it’s possible we might redesign our interface when we don’t need to. But if further testing shows that our iteration works, the cost of this false negative is only the time it took to do the redesign. Again, this false negative isn’t that costly, as long as we keep our iterations and our future testing small.

Additionally, when we run fast iterations, we are in a better position to make decisions using multiple data points from several tests rather than make decisions based on a single data point. We can test if our subscribers want to watch sports through a number of testing methods. We can ask them about their past viewing behavior and see if they have watched sports in the past. We can show them a mockup and ask them what they would like to watch right now. We can simulate the moment before the big game and see if they choose our service. Instead of throwing out an assumption based on one data point, we can draw conclusions from the set of assumption tests. Researchers call this triangulation.52 It’s using a mix of research methods to better understand the assumption we are testing.

Finally, even in the worst-case scenario, when we do decide to abandon an idea or an opportunity and it turns out it was based on a false negative, it’s still okay. There are hundreds, if not thousands, of ideas that could address our target opportunity or opportunities that could drive our desired outcome. When we throw one away needlessly, it’s not that costly, as long as we find an idea that does work or an opportunity that does have an impact. Remember, there isn’t one right idea or one right opportunity. We can afford false negatives because ideas and opportunities are abundant.

Now let’s turn to false positives. A false positive is when our test gives us data suggesting that our assumption is true, when it isn’t. This sounds far riskier than a false negative, but, in practice, it’s not. Suppose we run our small test, and we learn that everyone wants to watch sports, so we call our test a success, and we move forward. Remember, we aren’t making a go/no-go decision based on one assumption test. We are either moving on to test another assumption related to the same idea, or we are running a bigger, more reliable test on the same assumption. If our idea really is faulty, odds are that our next round of assumption testing will catch it. False positives usually get surfaced in successive rounds of testing. The cost of a false positive in a small test is usually the time and effort required to run the next-bigger test. That’s not trivial, but we still avoid the far-bigger cost of building the wrong solutions.

I want to be clear: There is a cost to false negatives and false positives. And we should be aware that these costs exist. But the cost is not so great that we should be starting with large-scale, quantitative experiments every time. If we did that, we would never ship any software. Our tests would simply take too long. The vast majority of the time, you will learn plenty from your small-scale tests.

A Quick Word on Science

Science-minded readers might cringe at these quick-and-dirty research methods. However, product teams are not scientists. Scientists work to understand the fundamental laws of the universe. They are seeking truths, creating new knowledge. In science (and the rest of academia), truth is determined over decades. Research studies are designed and replicated by a community of scientists. Truth starts to emerge from a meta-analysis of years of research. Even then, our truths don’t always stand the test of time. Newton’s laws of physics are true in some contexts, but with quantum physics, we are learning there are contexts in which they are not true. Newton’s truths are incomplete. In social science, we think a landmark research study means one thing, until years of more research show there was a confounding variable that wasn’t accounted for. Social-science truths often evolve. Research is messy, and creating new knowledge is hard.

Product teams, fortunately, are not creating new knowledge. Instead, we are trying to create products that improve our customers’ lives. When we launch a new product or service, we get to see how our customers interact with it. This is a fantastic feedback loop. We quickly get to measure if our product had the intended impact. We work on much faster cycles than science. It took almost 100 years before we could collect physical evidence to support Einstein’s theory of general relativity.53

So, while we want to adopt a scientific mindset and we want to think about the reliability and the validity of the data that we collect, we are not running scientific experiments. While we need to be thoughtful about our research methods, we also need to be aware that we are not validating or invalidating anything. It’s important that we recognize that our research findings are not truths—they are merely confirming or disconfirming evidence that either supports or refutes our point of view. Our goal as a product team is not to seek truth but to mitigate risk. We need to do just enough research to mitigate the risk that our companies cannot bear and no more.

This understanding of the intent behind our research will help us strike the right balance between sound research methods and the pace at which we need to work to get products into our customers’ hands.

Running Assumption Tests

The Identifying Hidden Assumptions chapter (Chapter 9) opened with a quote from Marty Cagan, in which he argued the best teams conduct 15–20 discovery iterations a week. This can sound like an overwhelming number of assumption tests. But with the right mindset, tools, and methods, it can quickly become a reality. In Chapter 9, we learned that the secret to unlocking this cadence is testing assumptions, not whole ideas. However, we still need to learn how to quickly execute our assumption tests.

There are two tools that should be in every product team’s toolbox—unmoderated user testing and one-question surveys. Unmoderated user-testing services allow you to post a stimulus (e.g., a prototype) and define tasks to complete and questions to answer. Participants then complete the tasks and answer the questions on their own time. You get a video of their work. These types of tools are game changers. Instead of having to recruit 10 participants and run the sessions yourself, you can post your task, go home for the night, and come back the next day to a set of videos ready for you to watch.

If we look at the two simulations we designed above, both could be conducted with unmoderated testing tools. Once results come in, we would simply have to watch the videos and record how many chose sports in the first assumption test and how many chose our subscription service in the second assumption test. What used to take weeks to recruit, schedule, and conduct a prototype test can now be done in a day or two.

To make unmoderated testing work well, you need to be thoughtful about who you recruit. With both of our tests, we need to recruit our own subscribers. So, we’ll need to screen for this. We also want to pay particular attention to variation (as discussed above). This is also something we can screen for. Some unmoderated testing tools also allow you to upload your own list of participants. This is particularly helpful when testing with niche audiences.

Many assumptions can be tested with quick answers to a single question. This is where one-question survey tools can be tremendously helpful. If we wanted to test the “Our subscribers want to watch sports” assumption in more than one way, we could launch a one-question survey asking them, “When was the last time you watched a sporting event?” We could use their answers to triangulate with our prototype simulation.

Sometimes we simply need to learn about our customers’ preferences. For example, if we were testing the assumption “Our platform has the sports our subscribers want to watch,” we could test this with a one-question survey. We could ask, “Please select all the sports you’ve watched in the past month.”

When using one-question surveys, we want to make sure we are following the same research rules we’ve outlined before. When asking about past behavior, we want to ask about specific instances (as you learned in Chapter 5). So, we are asking about the last week and the last month, not in general. We also want to avoid asking about what they might do in the future. We know this leads to unreliable data.

Sometimes you can use one-question surveys to simulate an experience. For example, if one of our ideas depends on the assumption “Our subscribers will tell us who their favorite sports teams are,” you might be tempted to ask customers, “Are you willing to tell us who your favorite sports teams are?” But this is a question about future behavior. The answers are unreliable. Instead, ask, “What are your favorite sports teams?” Evaluate the results based on the percentage of people who answer as compared to the percentage of people who skipped it.

However, unmoderated testing and one-question surveys aren’t the only ways to test assumptions. Sometimes we already have the data we need in our own database. For example, we might look at how many of our current subscribers have searched for sports on our platform and use this as an indicator of interest in sports. Before you dive into the data, be sure to define your evaluation criteria upfront. How many search queries will you sample? How many need to be related to sports? How will you determine “related to sports”? Remember, aligning around success criteria upfront guards against confirmation bias and ensures that your team agrees on what the results mean.

Product teams can typically test most of their assumptions with a combination of prototype tests (either unmoderated or in person), one-question surveys, or through data-mining. However, there are dozens of experiment types. If you want to do a deep dive on qualitative tests, pick up a copy of Laura Klein’s UX for Lean Startups. She does a good job of surveying a wide breadth of methods. Another great reference is David Bland’s Testing Business Ideas. The last third of David’s book is an encyclopedia of experiment types. However, don’t get overwhelmed with having to master all of these experiment types. If you keep the simple assumption-simulate-evaluate framework in mind, you’ll be well on your way to becoming a strong assumption tester.

Avoid These Common Anti-Patterns

As you design and run your assumption tests, keep these common anti-patterns in mind:

Overly complex simulations. Some teams spend countless hours, days, or even weeks trying to design and develop the perfect simulation. It’s easy to lose sight of the goal. In your first round of testing, you are looking to design fast tests that will help you gather quick signals. Design your tests to be completed in a day or two, or a week, at most. This will ensure that you can keep your discovery iterations high.

Using percentages instead of specific numbers when defining evaluation criteria. Many teams equate 70% and 7 out of 10. So instead of defining their evaluation criteria as 7 out of 10, they tend to favor the percentage. These sound equivalent, but they aren’t. First, when testing with small numbers, we can’t conclude that 7 out of 10 will continue to mean 70% as our participant size grows. We want to make sure that we don’t draw too strong a conclusion from our small signals. Second, and more importantly, “70%” is ambiguous. If we test with 10 people and only 6 exhibit our desired behavior, some of us might conclude that the test failed. Others might argue that we need to test with more people. Be explicit from the get-go about how many people you will test with when defining your success criteria.

Not defining enough evaluation criteria. It’s easy to forget important evaluation criteria. At a minimum, you need to define how many people to test with and how many will exhibit the desired behavior. But for some tests, defining the desired behavior may involve more than one number. For example, if your test involves sending an email, you might need to define how many people will receive the email, how long you’ll give them to open the email, and whether your success criteria is “opens” or “clicks.” Pay particular attention to the success threshold. Complex actions may require multiple measurements (e.g., opens the email, clicks on the link, takes an action).

Testing with the wrong audience. Make sure that you are testing with the right people. If you are testing solutions for a specific target opportunity, make sure that your participants experience the need, pain point, or desire represented by that target opportunity. Remember to recruit for variation. Don’t just test with the easiest audience to reach or the most vocal audience.

Designing for less than the best-case scenario. When testing with small numbers, design your assumption tests such that they are likely to pass. If your assumption test passes with the most likely audience, then you can expand your reach to tougher audiences. This might feel like cheating, but you’ll be surprised how often your assumption tests still fail. If you fail in the best-case scenario, your results will be less ambiguous. If your test fails with a less-than-ideal audience, someone on the team is going to argue you tested with the wrong audience, and you’ll have to run the test again. Remember, we want to design our tests to learn as much as we can from failures.